{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e428336",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os, re, json, shutil, datetime, zipfile\n",
        "from pathlib import Path\n",
        "from convokit import Corpus, Speaker, Utterance, Conversation\n",
        "from collections import Counter\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0687b61b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the ConvoKit corpus\n",
        "corpus = Corpus(filename=\"./dialog_corpus\")\n",
        "print(f\"Number of conversations: {len(list(corpus.iter_conversations()))}\")\n",
        "print(f\"Number of utterances: {len(list(corpus.iter_utterances()))}\")\n",
        "print(f\"Number of speakers: {len(list(corpus.iter_speakers()))}\")\n",
        "\n",
        "# Show corpus metadata\n",
        "print(\"\\n=== Corpus Metadata ===\")\n",
        "print(corpus.meta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b82ed8e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Randomly output several conversations and utterances to show data structure\n",
        "\n",
        "import random\n",
        "\n",
        "# Print structure of several conversations\n",
        "print(\"\\n=== Sample Conversations ===\")\n",
        "all_convs = list(corpus.iter_conversations())\n",
        "sample_convs = random.sample(all_convs, min(3, len(all_convs)))\n",
        "for conv in sample_convs:\n",
        "    print(f\"Conversation ID: {conv.id}\")\n",
        "    print(f\"Meta: {conv.meta}\")\n",
        "    print(f\"Number of Utterances: {len(conv.get_utterance_ids())}\")\n",
        "    print(\"Utterance IDs:\", conv.get_utterance_ids()[:3], \"...\")\n",
        "    print()\n",
        "\n",
        "# Print structure of several utterances\n",
        "print(\"\\n=== Sample Utterances ===\")\n",
        "all_utts = list(corpus.iter_utterances())\n",
        "sample_utts = random.sample(all_utts, min(3, len(all_utts)))\n",
        "for utt in sample_utts:\n",
        "    print(f\"Utterance ID: {utt.id}\")\n",
        "    print(f\"Speaker: {utt.speaker.id if utt.speaker else None}\")\n",
        "    print(f\"Text: {utt.text[:60]}\" + (\"...\" if len(utt.text) > 60 else \"\"))\n",
        "    print(f\"Conversation ID: {utt.conversation_id}\")\n",
        "    print(f\"Reply to: {utt.reply_to}\")\n",
        "    print(f\"Meta: {utt.meta}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd52e458",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# Read all JSON files from GPT folder, replace 'seeker' in id with 'utterance'\n",
        "sb_data = {}\n",
        "gpt_dir = \"./GPT\"\n",
        "\n",
        "print(\"Reading SB scores from GPT files...\")\n",
        "for filename in os.listdir(gpt_dir):\n",
        "    if filename.endswith('.json'):\n",
        "        filepath = os.path.join(gpt_dir, filename)\n",
        "        with open(filepath, 'r') as f:\n",
        "            data = json.load(f)\n",
        "            for item in data:\n",
        "                orig_id = item['id']  # e.g. \"seeker_0_2\"\n",
        "                utt_id = orig_id.replace('seeker', 'utterance')\n",
        "                sb_value = item['SB']\n",
        "                sb_data[utt_id] = sb_value\n",
        "\n",
        "print(json.dumps(sb_data, ensure_ascii=False, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "735b8e17",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "gpt_dir = \"./GPT\"\n",
        "file_count = 0\n",
        "total_item_count = 0\n",
        "file_item_counts = {}\n",
        "\n",
        "for filename in os.listdir(gpt_dir):\n",
        "    if filename.endswith('.json'):\n",
        "        filepath = os.path.join(gpt_dir, filename)\n",
        "        file_count += 1\n",
        "        with open(filepath, \"r\") as f:\n",
        "            try:\n",
        "                data = json.load(f)\n",
        "                item_count = len(data)\n",
        "                file_item_counts[filename] = item_count\n",
        "                total_item_count += item_count\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {filename}: {e}\")\n",
        "\n",
        "print(f\"Total JSON files in GPT folder: {file_count}\")\n",
        "for fname, cnt in file_item_counts.items():\n",
        "    print(f\"{fname} contains {cnt} items (seeker_x_y)\")\n",
        "print(f\"Total items across all JSON files: {total_item_count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e446bc44",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read intent data from utterances_intent.jsonl\n",
        "intent_data = {}\n",
        "\n",
        "print(\"Loading intent data from utterances_intent.jsonl...\")\n",
        "with open('./utterances_intent.jsonl', 'r') as f:\n",
        "    for line in f:\n",
        "        data = json.loads(line.strip())\n",
        "        utterance_id = data['id']\n",
        "        intent_data[utterance_id] = {\n",
        "            'intent': data['intent'],\n",
        "            'intent_confidence': data['confidence']\n",
        "        }\n",
        "\n",
        "print(f\"Loaded intent data for {len(intent_data)} utterances\")\n",
        "print(f\"Sample intent data: {list(intent_data.items())[:1]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "467e420d",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Number of SB data entries: {len(sb_data)}\")\n",
        "print(f\"Number of intent data entries: {len(intent_data)}\")\n",
        "if len(sb_data) == len(intent_data):\n",
        "    print(\"sb_data and intent_data have the same number of entries.\")\n",
        "else:\n",
        "    print(\"sb_data and intent_data have DIFFERENT number of entries.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "224fdcce",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check why keys (utterance_id) in sb_data and intent_data are different\n",
        "from collections import defaultdict\n",
        "\n",
        "# Get key sets of both\n",
        "sb_keys = set(sb_data.keys())\n",
        "intent_keys = set(intent_data.keys())\n",
        "\n",
        "# utterance_ids in sb_data but not in intent_data, and vice versa\n",
        "sb_only = sb_keys - intent_keys\n",
        "intent_only = intent_keys - sb_keys\n",
        "\n",
        "print(f\"Number of utterance_ids in sb_data but not in intent_data: {len(sb_only)}\")\n",
        "if sb_only:\n",
        "    print(f\"Examples (first 10): {list(sb_only)[:10]}\")\n",
        "print(f\"Number of utterance_ids in intent_data but not in sb_data: {len(intent_only)}\")\n",
        "if intent_only:\n",
        "    print(f\"Examples (first 10): {list(intent_only)[:10]}\")\n",
        "\n",
        "# Analyze id suffixes, conversation_id and local_utt_id\n",
        "def analyze_ids(id_set, label):\n",
        "    conv2utts = defaultdict(list)\n",
        "    for uid in id_set:\n",
        "        try:\n",
        "            parts = uid.split('_')\n",
        "            conv = parts[1]\n",
        "            local = int(parts[2])\n",
        "            conv2utts[conv].append(local)\n",
        "        except Exception as e:\n",
        "            print(f\"{label}: Cannot parse utterance_id: {uid}, error: {e}\")\n",
        "    # Analyze if there are gaps in utterance local ids within conversations\n",
        "    summary = {}\n",
        "    for conv, utt_ids in conv2utts.items():\n",
        "        sorted_ids = sorted(utt_ids)\n",
        "        missing = []\n",
        "        if sorted_ids:\n",
        "            for i in range(sorted_ids[0], sorted_ids[-1]):\n",
        "                if i not in utt_ids:\n",
        "                    missing.append(i)\n",
        "        if missing:\n",
        "            summary[conv] = missing\n",
        "    print(f\"{label} - gaps in local_utt_id within conversations (showing only those with gaps):\")\n",
        "    if summary:\n",
        "        for conv, missing in list(summary.items())[:5]:  # Show at most first 5\n",
        "            print(f\"  conversation {conv} missing local_utt_ids: {missing}\")\n",
        "    else:\n",
        "        print(\"  No obvious gaps found.\")\n",
        "\n",
        "print(\"==== Analysis of sb_data keys (utterance_id) ====\")\n",
        "analyze_ids(sb_keys, \"sb_data\")\n",
        "print(\"==== Analysis of intent_data keys (utterance_id) ====\")\n",
        "analyze_ids(intent_keys, \"intent_data\")\n",
        "\n",
        "# Compare distributions of sb_only and intent_only to see if some conversations are completely skipped\n",
        "def count_conversation(ids):\n",
        "    counts = defaultdict(int)\n",
        "    for uid in ids:\n",
        "        try:\n",
        "            parts = uid.split('_')\n",
        "            conv = parts[1]\n",
        "            counts[conv] += 1\n",
        "        except Exception:\n",
        "            continue\n",
        "    return counts\n",
        "\n",
        "print(\"\\nsb_data only conversation distribution: (showing at most first 10)\")\n",
        "for conv,count in list(count_conversation(sb_only).items())[:10]:\n",
        "    print(f\"  conversation {conv} : {count} utterances\")\n",
        "\n",
        "print(\"\\nintent_data only conversation distribution: (showing at most first 10)\")\n",
        "for conv,count in list(count_conversation(intent_only).items())[:10]:\n",
        "    print(f\"  conversation {conv} : {count} utterances\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5448dd1c",
      "metadata": {},
      "outputs": [],
      "source": [
        "sb_data['utterance_0_18']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da57d06d",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d53e1f2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Map SB continuous values to binary labels (threshold 0.7)\n",
        "sb_binary = {}\n",
        "for k, v in sb_data.items():\n",
        "    try:\n",
        "        sb_binary[k] = 1 if v >= 0.7 else 0\n",
        "    except Exception:\n",
        "        sb_binary[k] = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7695f263",
      "metadata": {},
      "outputs": [],
      "source": [
        "sb_binary['utterance_0_8']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "298252b7",
      "metadata": {},
      "outputs": [],
      "source": [
        "len(sb_binary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8db36592",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Keep only IDs that exist in both sb_binary and intent_data, and integrate\n",
        "# Each ID has three labels: intent, intent_confidence, sb_binary\n",
        "\n",
        "# Take only IDs that exist in both sb_binary and intent_data (intersection)\n",
        "common_ids = set(sb_binary.keys()) & set(intent_data.keys())\n",
        "print(f\"Number of IDs in both sb_binary and intent_data: {len(common_ids)}\")\n",
        "\n",
        "integrated_labels = {}\n",
        "for uid in common_ids:\n",
        "    # sb_binary value is already 0 or 1\n",
        "    sb_val = sb_binary.get(uid)\n",
        "    intent_info = intent_data.get(uid)\n",
        "    \n",
        "    if sb_val is not None and intent_info is not None:\n",
        "        intent_val = intent_info.get(\"intent\")\n",
        "        confidence_val = intent_info.get(\"intent_confidence\")  # Note: key is 'intent_confidence' not 'confidence'\n",
        "        \n",
        "        if intent_val is not None and confidence_val is not None:\n",
        "            integrated_labels[uid] = {\n",
        "                \"utterance_id\": uid,\n",
        "                \"intent\": intent_val,\n",
        "                \"intent_confidence\": confidence_val,\n",
        "                \"sb_binary\": sb_val\n",
        "            }\n",
        "\n",
        "print(f\"\\nNumber of successfully integrated data points: {len(integrated_labels)}\")\n",
        "\n",
        "# Show first few examples\n",
        "if integrated_labels:\n",
        "    print(\"\\nFirst 5 example data points:\")\n",
        "    for i, (uid, data) in enumerate(list(integrated_labels.items())[:5]):\n",
        "        print(f\"{i+1}. {data}\")\n",
        "else:\n",
        "    print(\"No utterance_ids found that exist in both sb_binary and intent_data\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6545a29",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert integrated_labels to DataFrame and split ID\n",
        "import pandas as pd\n",
        "\n",
        "# Convert integrated_labels to DataFrame and split conversation_id, turn_id as shown in example\n",
        "df = pd.DataFrame(list(integrated_labels.values()))\n",
        "\n",
        "# Add conversation_id and turn_id fields (split based on utterance_id format \"utterance_{conv_id}_{turn_id}\")\n",
        "ids = df[\"utterance_id\"].str.extract(r\"utterance_(\\d+)_(\\d+)\")\n",
        "df[\"conv_id\"] = ids[0].astype(int)\n",
        "df[\"turn_id\"] = ids[1].astype(int)\n",
        "\n",
        "# To view first few rows\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c74b7e01",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3319c669",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encode intent as integers, then do one-hot\n",
        "intent2id = {intent: i for i, intent in enumerate(sorted(df[\"intent\"].unique()))}\n",
        "df[\"intent_id\"] = df[\"intent\"].map(intent2id)\n",
        "\n",
        "num_intents = len(intent2id)\n",
        "print(\"Num intents:\", num_intents)\n",
        "\n",
        "# Keep only columns we need\n",
        "df = df[[\"utterance_id\", \"conv_id\", \"turn_id\", \"intent_id\",\n",
        "         \"intent_confidence\", \"sb_binary\"]]\n",
        "\n",
        "# Sort by conversation and order\n",
        "df = df.sort_values([\"conv_id\", \"turn_id\"]).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33a66c9f",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a376d42c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Get all conversation IDs\n",
        "all_convs = df[\"conv_id\"].unique()\n",
        "\n",
        "# Split by conversation\n",
        "train_convs, test_convs = train_test_split(all_convs, test_size=0.2, random_state=42)\n",
        "train_convs, val_convs = train_test_split(train_convs, test_size=0.1, random_state=42)\n",
        "\n",
        "train_df = df[df[\"conv_id\"].isin(train_convs)]\n",
        "val_df = df[df[\"conv_id\"].isin(val_convs)]\n",
        "test_df = df[df[\"conv_id\"].isin(test_convs)]\n",
        "\n",
        "len(train_df), len(val_df), len(test_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f30fda6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_balance(name, df):\n",
        "    print(f\"=== {name} ===\")\n",
        "    print(df[\"sb_binary\"].value_counts())\n",
        "    print(df[\"sb_binary\"].value_counts(normalize=True))\n",
        "\n",
        "show_balance(\"TRAIN\", train_df)\n",
        "show_balance(\"VAL\", val_df)\n",
        "show_balance(\"TEST\", test_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42a2ed95",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# ===== 1. Select baseline input features =====\n",
        "baseline_features = [\"intent_id\", \"intent_confidence\", \"turn_id\"]  # If you have turn_id_norm, change it to that\n",
        "\n",
        "X_train = train_df[baseline_features].values\n",
        "X_val   = val_df[baseline_features].values\n",
        "X_test  = test_df[baseline_features].values\n",
        "\n",
        "y_train = train_df[\"sb_binary\"].values\n",
        "y_val   = val_df[\"sb_binary\"].values\n",
        "y_test  = test_df[\"sb_binary\"].values\n",
        "\n",
        "# ===== 2. Feature standardization (especially turn_id) =====\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val   = scaler.transform(X_val)\n",
        "X_test  = scaler.transform(X_test)\n",
        "\n",
        "# ===== 3. Define Logistic Regression to handle class imbalance =====\n",
        "clf = LogisticRegression(\n",
        "    class_weight=\"balanced\",  # Automatically give higher weight to minority class (SB=1)\n",
        "    max_iter=500,\n",
        "    solver=\"lbfgs\"\n",
        ")\n",
        "\n",
        "# ===== 4. Train =====\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "baseline_results = {}\n",
        "\n",
        "# ===== 5. Evaluate on train / val / test =====\n",
        "def eval_split(X, y, name):\n",
        "    y_pred = clf.predict(X)\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    print(classification_report(y, y_pred, digits=3))\n",
        "    acc = accuracy_score(y, y_pred)\n",
        "    macro = f1_score(y, y_pred, average=\"macro\")\n",
        "    pos_f1 = f1_score(y, y_pred, pos_label=1)\n",
        "    print(\"Accuracy:\", acc)\n",
        "    print(\"Macro F1:\", macro)\n",
        "    print(\"Pos-class (SB=1) F1:\", pos_f1)\n",
        "\n",
        "    baseline_results[name] = {\n",
        "        \"accuracy\": acc,\n",
        "        \"macro_f1\": macro,\n",
        "        \"pos_f1\": pos_f1,\n",
        "    }\n",
        "    return baseline_results[name]\n",
        "\n",
        "eval_split(X_train, y_train, \"TRAIN\")\n",
        "eval_split(X_val, y_val, \"VAL\")\n",
        "# Run evaluation first\n",
        "test_metrics = eval_split(X_test, y_test, \"TEST\")\n",
        "\n",
        "# Convert results to table\n",
        "import pandas as pd\n",
        "result_table = pd.DataFrame([\n",
        "    {\n",
        "        \"split\": \"train\",\n",
        "        **baseline_results[\"TRAIN\"]\n",
        "    },\n",
        "    {\n",
        "        \"split\": \"val\",\n",
        "        **baseline_results[\"VAL\"]\n",
        "    },\n",
        "    {\n",
        "        \"split\": \"test\",\n",
        "        **baseline_results[\"TEST\"]\n",
        "    }\n",
        "])\n",
        "\n",
        "print(\"\\nBaseline results table:\")\n",
        "display(result_table)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26d4b7c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import copy\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
        "\n",
        "# ---------- First ensure turn_id_norm exists ----------\n",
        "for split_df in [train_df, val_df, test_df]:\n",
        "    split_df[\"turn_id_norm\"] = split_df.groupby(\"conv_id\")[\"turn_id\"].transform(\n",
        "        lambda x: x / x.max() if x.max() > 0 else 0\n",
        "    )\n",
        "\n",
        "# Range of all intents\n",
        "num_intents = int(max(\n",
        "    train_df[\"intent_id\"].max(),\n",
        "    val_df[\"intent_id\"].max(),\n",
        "    test_df[\"intent_id\"].max()\n",
        ")) + 1\n",
        "print(\"num_intents:\", num_intents)\n",
        "\n",
        "\n",
        "def one_hot_intent(intent_ids, num_intents):\n",
        "    \"\"\"intent_ids: 1D array of ints -> 2D one-hot matrix\"\"\"\n",
        "    N = len(intent_ids)\n",
        "    oh = np.zeros((N, num_intents), dtype=np.float32)\n",
        "    oh[np.arange(N), intent_ids.astype(int)] = 1.0\n",
        "    return oh\n",
        "\n",
        "\n",
        "def build_sequence_data(split_df, K=5):\n",
        "    \"\"\"\n",
        "    For a split (train/val/test), construct:\n",
        "      X: (num_samples, seq_len=K+1, feature_dim)\n",
        "      y: (num_samples,)\n",
        "    \n",
        "    Features for each timestep:\n",
        "      [ sb_binary, intent_onehot..., intent_confidence, turn_id_norm ]\n",
        "    For current utterance t:\n",
        "      sb_binary_t uses 0 as placeholder (to avoid label leakage)\n",
        "    When history has fewer than K items, use left zero padding.\n",
        "    \"\"\"\n",
        "    all_X = []\n",
        "    all_y = []\n",
        "\n",
        "    # Group by conversation\n",
        "    for conv_id, g in split_df.groupby(\"conv_id\"):\n",
        "        g = g.sort_values(\"turn_id\").reset_index(drop=True)\n",
        "\n",
        "        intents = g[\"intent_id\"].values\n",
        "        confs = g[\"intent_confidence\"].values.astype(np.float32)\n",
        "        sbs    = g[\"sb_binary\"].values.astype(np.float32)\n",
        "        pos    = g[\"turn_id_norm\"].values.astype(np.float32)\n",
        "\n",
        "        # First compute one-hot for each time step\n",
        "        intents_oh = one_hot_intent(intents, num_intents)  # (N, num_intents)\n",
        "\n",
        "        N = len(g)\n",
        "        for t in range(N):\n",
        "            # Label for current utterance\n",
        "            y_t = sbs[t]\n",
        "\n",
        "            # Take history window [t-K, ..., t]\n",
        "            start = max(0, t - K)\n",
        "            end = t  # t itself is the current utterance, we handle it separately\n",
        "\n",
        "            # Indices for history part: start..t-1\n",
        "            hist_idx = list(range(start, t))  # May be empty\n",
        "\n",
        "            hist_feats = []\n",
        "            for idx in hist_idx:\n",
        "                feat = np.concatenate([\n",
        "                    np.array([sbs[idx]]),          # Historical SB\n",
        "                    intents_oh[idx],               # intent one-hot\n",
        "                    np.array([confs[idx]]),\n",
        "                    np.array([pos[idx]]),\n",
        "                ], axis=0)\n",
        "                hist_feats.append(feat)\n",
        "\n",
        "            # Current utterance features (note: do not include sb_binary_t)\n",
        "            curr_feat = np.concatenate([\n",
        "                np.array([0.0]),                  # Current utterance sb set to 0 as placeholder\n",
        "                intents_oh[t],\n",
        "                np.array([confs[t]]),\n",
        "                np.array([pos[t]]),\n",
        "            ], axis=0)\n",
        "\n",
        "            # Concatenate history + current into sequence, length <= K+1\n",
        "            seq_feats = hist_feats + [curr_feat]  # length = len(hist_idx) + 1\n",
        "\n",
        "            # Left padding to fixed length K+1\n",
        "            feat_dim = curr_feat.shape[0]\n",
        "            pad_len = (K + 1) - len(seq_feats)\n",
        "            if pad_len > 0:\n",
        "                pad = [np.zeros(feat_dim, dtype=np.float32) for _ in range(pad_len)]\n",
        "                seq_feats = pad + seq_feats\n",
        "\n",
        "            X_t = np.stack(seq_feats, axis=0)  # (K+1, feat_dim)\n",
        "            all_X.append(X_t)\n",
        "            all_y.append(y_t)\n",
        "\n",
        "    X = np.stack(all_X, axis=0).astype(np.float32)  # (num_samples, K+1, feat_dim)\n",
        "    y = np.array(all_y).astype(np.float32)          # (num_samples,)\n",
        "\n",
        "    print(\"build_sequence_data: X.shape =\", X.shape, \" y.mean(SB=1 rate) =\", y.mean())\n",
        "    return X, y\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e643501c",
      "metadata": {},
      "outputs": [],
      "source": [
        "K = 0  # Use past 5 turns + current turn as context\n",
        "X_train, y_train = build_sequence_data(train_df, K=K)\n",
        "X_val,   y_val   = build_sequence_data(val_df,   K=K)\n",
        "X_test,  y_test  = build_sequence_data(test_df,  K=K)\n",
        "\n",
        "seq_len = X_train.shape[1]\n",
        "feat_dim = X_train.shape[2]\n",
        "print(\"seq_len =\", seq_len, \" feat_dim =\", feat_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ba9a2df",
      "metadata": {},
      "outputs": [],
      "source": [
        "class SBSeqDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "\n",
        "batch_size = 64  # Can be adjusted smaller/larger\n",
        "\n",
        "train_loader = DataLoader(SBSeqDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(SBSeqDataset(X_val,   y_val),   batch_size=batch_size)\n",
        "test_loader  = DataLoader(SBSeqDataset(X_test,  y_test),  batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cf63556",
      "metadata": {},
      "outputs": [],
      "source": [
        "class GRUSBModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=32, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, 1)  # Output a logit, BCE with logits\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, input_dim)\n",
        "        _, h_n = self.gru(x)       # h_n: (num_layers, batch, hidden_dim)\n",
        "        h_last = h_n[-1]           # (batch, hidden_dim)\n",
        "        logit = self.fc(h_last)    # (batch, 1)\n",
        "        return logit.squeeze(-1)   # (batch,)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21691ba1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate pos_weight = (#neg / #pos)\n",
        "num_pos = (y_train == 1).sum()\n",
        "num_neg = (y_train == 0).sum()\n",
        "pos_weight = torch.tensor([num_neg / num_pos], dtype=torch.float32)\n",
        "print(\"num_pos:\", num_pos, \" num_neg:\", num_neg, \" pos_weight:\", pos_weight.item())\n",
        "\n",
        "device = torch.device(\"cpu\")  # You can change to \"cuda\" if you have GPU\n",
        "model = GRUSBModel(input_dim=feat_dim, hidden_dim=32).to(device)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device))\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e18f5e8d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def eval_model(model, data_loader, device, split_name=\"VAL\", threshold=0.7):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in data_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            logits = model(X_batch)\n",
        "            probs = torch.sigmoid(logits)\n",
        "\n",
        "            all_labels.append(y_batch.cpu().numpy())\n",
        "            all_probs.append(probs.cpu().numpy())\n",
        "\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "    all_probs  = np.concatenate(all_probs)\n",
        "    all_preds  = (all_probs >= threshold).astype(int)\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "    pos_f1 = f1_score(all_labels, all_preds, pos_label=1)\n",
        "\n",
        "    print(f\"\\n=== {split_name} ===\")\n",
        "    print(classification_report(all_labels, all_preds, digits=3))\n",
        "    print(\"Accuracy:\", acc)\n",
        "    print(\"Macro F1:\", macro)\n",
        "    print(\"Pos-class (SB=1) F1:\", pos_f1)\n",
        "\n",
        "    metrics = {\n",
        "        \"accuracy\": acc,\n",
        "        \"macro_f1\": macro,\n",
        "        \"pos_f1\": pos_f1,\n",
        "    }\n",
        "    return all_labels, all_probs, all_preds, metrics\n",
        "\n",
        "\n",
        "num_epochs = 10  # Can try 10 epochs first, add more if needed\n",
        "\n",
        "best_macro_f1 = 0.0\n",
        "best_epoch = 0\n",
        "best_model_state = None\n",
        "best_val_metrics = None\n",
        "training_history = []\n",
        "best_model_path = f\"best_grusb_model_K{K}.pth\"\n",
        "\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "\n",
        "        logits = model(X_batch)\n",
        "        loss = criterion(logits, y_batch)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * X_batch.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader.dataset)\n",
        "    print(f\"Epoch {epoch}/{num_epochs} - Train Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Check on VAL each epoch\n",
        "    _, _, _, val_metrics = eval_model(model, val_loader, device, split_name=\"VAL\")\n",
        "\n",
        "    training_history.append({\n",
        "        \"epoch\": epoch,\n",
        "        \"train_loss\": avg_loss,\n",
        "        \"val_accuracy\": val_metrics[\"accuracy\"],\n",
        "        \"val_macro_f1\": val_metrics[\"macro_f1\"],\n",
        "        \"val_pos_f1\": val_metrics[\"pos_f1\"],\n",
        "    })\n",
        "\n",
        "    if val_metrics[\"macro_f1\"] > best_macro_f1:\n",
        "        best_macro_f1 = val_metrics[\"macro_f1\"]\n",
        "        best_epoch = epoch\n",
        "        best_model_state = copy.deepcopy(model.state_dict())\n",
        "        best_val_metrics = val_metrics.copy()\n",
        "        print(f\"[BEST] Saved best model at epoch {epoch} (macro F1={best_macro_f1:.4f})\")\n",
        "\n",
        "# Save best model to a file after training\n",
        "if best_model_state is not None:\n",
        "    torch.save(best_model_state, best_model_path)\n",
        "    model.load_state_dict(best_model_state)\n",
        "    print(\n",
        "        f\"Best model saved from epoch {best_epoch} with macro F1={best_macro_f1:.4f} to '{best_model_path}'\"\n",
        "    )\n",
        "else:\n",
        "    print(\"Warning: best model state was not set; using final epoch weights.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a13ceaa8",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "\n",
        "training_history_df = pd.DataFrame(training_history)\n",
        "history_csv_path = f\"final_project_data/training_history_K{K}.csv\"\n",
        "training_history_df.to_csv(history_csv_path, index=False)\n",
        "\n",
        "print(f\"Training history saved to {history_csv_path}\")\n",
        "display(training_history_df)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "ax1.plot(training_history_df[\"epoch\"], training_history_df[\"train_loss\"], marker='o', label='Train Loss')\n",
        "ax1.set_title('Training Loss per Epoch')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.legend()\n",
        "\n",
        "ax2.plot(training_history_df[\"epoch\"], training_history_df[\"val_accuracy\"], marker='o', label='Val Accuracy')\n",
        "ax2.plot(training_history_df[\"epoch\"], training_history_df[\"val_macro_f1\"], marker='o', label='Val Macro F1')\n",
        "ax2.plot(training_history_df[\"epoch\"], training_history_df[\"val_pos_f1\"], marker='o', label='Val Pos-class F1')\n",
        "ax2.set_title('Validation Metrics per Epoch')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Score')\n",
        "ax2.set_ylim(0.45, 0.8)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f29e8201",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n========== FINAL TEST EVAL ==========\")\n",
        "eval_model(model, test_loader, device, split_name=\"TEST\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed2c09d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load persisted test split (raise error if not found)\n",
        "test_split_path = \"final_project_data/test_split.csv\"\n",
        "if not os.path.exists(test_split_path):\n",
        "    raise FileNotFoundError(f\"Saved test split not found at {test_split_path}. Run the split-saving cell first.\")\n",
        "\n",
        "test_df_saved = pd.read_csv(test_split_path)\n",
        "# Recalculate turn_id_norm to ensure consistency with training phase\n",
        "test_df_saved[\"turn_id_norm\"] = test_df_saved.groupby(\"conv_id\")[\"turn_id\"].transform(\n",
        "    lambda x: x / x.max() if x.max() > 0 else 0\n",
        ")\n",
        "\n",
        "def evaluate_saved_models(k_values, test_df_source, batch_size=64, hidden_dim=32, threshold=0.7):\n",
        "    results = []\n",
        "\n",
        "    for k in k_values:\n",
        "        model_path = f\"best_grusb_model_K{k}.pth\"\n",
        "        print(f\"\\n--- Evaluating saved model: K={k} ({model_path}) ---\")\n",
        "        if not os.path.exists(model_path):\n",
        "            print(f\"[SKIP] Checkpoint not found: {model_path}\")\n",
        "            continue\n",
        "\n",
        "        # Build test data for this K (based on persisted test split)\n",
        "        X_test_k, y_test_k = build_sequence_data(test_df_source.copy(), K=k)\n",
        "        test_loader_k = DataLoader(SBSeqDataset(X_test_k, y_test_k), batch_size=batch_size)\n",
        "\n",
        "        # Create model and load weights\n",
        "        model_k = GRUSBModel(input_dim=X_test_k.shape[2], hidden_dim=hidden_dim).to(device)\n",
        "        state_dict = torch.load(model_path, map_location=device)\n",
        "        model_k.load_state_dict(state_dict)\n",
        "\n",
        "        # Evaluate on test set\n",
        "        _, _, _, metrics = eval_model(\n",
        "            model_k,\n",
        "            test_loader_k,\n",
        "            device,\n",
        "            split_name=f\"TEST (K={k})\",\n",
        "            threshold=threshold,\n",
        "        )\n",
        "\n",
        "        results.append({\n",
        "            \"K\": k,\n",
        "            \"accuracy\": metrics[\"accuracy\"],\n",
        "            \"macro_f1\": metrics[\"macro_f1\"],\n",
        "            \"pos_f1\": metrics[\"pos_f1\"],\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results).sort_values(\"K\").reset_index(drop=True)\n",
        "\n",
        "\n",
        "k_range = list(range(0, 9))  # K=1..8\n",
        "eval_batch_size = 64\n",
        "gru_eval_df = evaluate_saved_models(\n",
        "    k_range,\n",
        "    test_df_source=test_df_saved,\n",
        "    batch_size=eval_batch_size,\n",
        "    hidden_dim=32,\n",
        "    threshold=0.7,\n",
        ")\n",
        "\n",
        "if not gru_eval_df.empty:\n",
        "    csv_path = \"final_project_data/gru_eval_across_K.csv\"\n",
        "    gru_eval_df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\nSaved evaluation summary to {csv_path}\")\n",
        "    display(gru_eval_df)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    for metric in [\"accuracy\", \"macro_f1\", \"pos_f1\"]:\n",
        "        plt.plot(gru_eval_df[\"K\"], gru_eval_df[metric], marker='o', label=metric)\n",
        "    plt.title(\"GRU Performance vs K (best checkpoints)\")\n",
        "    plt.xlabel(\"K (history length)\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.ylim(0.45, 0.85)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No saved checkpoints were evaluated. Please ensure best_grusb_model_K*.pth files exist.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4bf7398",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
